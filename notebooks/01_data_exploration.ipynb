{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17fd7557",
   "metadata": {},
   "source": [
    "# Explortatory Data Analysis for Scalable-Credit-Card-Fraud-Detection-Behavior-Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8566778f-2129-4651-8b49-ecf491b11d58",
   "metadata": {},
   "source": [
    "\n",
    "This notebook will focus on understanding the structure and content of the datasets we are using the project.The objective is to explore the transactions, cards, and users data to identify the key variables, data quality issues, and  other initial patterns that may be useful for fraud detection.\n",
    "\n",
    "No cleaning or feature engineering is performed at this stage.\n",
    "\n",
    "The notebook is formatted based on the steps followed in the  EDA report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a9ce337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "session = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Credit Card Fraud Detection\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"6g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17124388",
   "metadata": {},
   "source": [
    "#### Load the Raw Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52319b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "transactions_df = session.read.csv(\"data/raw/transactions_data.csv\", header=True, inferSchema=True)\n",
    "cards_df = session.read.csv(\"data/raw/cards_data.csv\", header=True, inferSchema=True) \n",
    "users_df = session.read.csv(\"data/raw/users_data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b75842-e95f-44ed-af0a-cfe84277f55e",
   "metadata": {},
   "source": [
    "## Transactions Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b07441",
   "metadata": {},
   "source": [
    "The transactions dataset is significantly larger with more than 8 lac records whis confirms that transactions dataset is the main dataset, while cards and users provide contextual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11dd62ea-e412-4e1a-a4ee-62e558527371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'date',\n",
       " 'client_id',\n",
       " 'card_id',\n",
       " 'amount',\n",
       " 'use_chip',\n",
       " 'merchant_id',\n",
       " 'merchant_city',\n",
       " 'merchant_state',\n",
       " 'zip',\n",
       " 'mcc',\n",
       " 'errors']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd30987-598d-4a9b-8d4e-64a6cabe913e",
   "metadata": {},
   "source": [
    "The transactions dataset includes:\n",
    "- Identifiers (id, client_id, card_id)\n",
    "- Temporal information (date)\n",
    "- amount\n",
    "- use_chip\n",
    "- Merchant information(merchant_id,merchant_city,merchant_state)\n",
    "- mcc (Might be Merchant Country Code)\n",
    "- An error field that may indicate abnormal or failed transactions\n",
    "\n",
    "This dataset is the core of the fraud detection task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8078cb-8a03-45d4-910b-9e9529eb7964",
   "metadata": {},
   "source": [
    "### 1. Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "835e2c5c-4d26-464c-a861-f4c8914ac0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---------+-------+-------+------------------+-----------+-------------+--------------+-------+----+------+\n",
      "|     id|               date|client_id|card_id| amount|          use_chip|merchant_id|merchant_city|merchant_state|    zip| mcc|errors|\n",
      "+-------+-------------------+---------+-------+-------+------------------+-----------+-------------+--------------+-------+----+------+\n",
      "|7475327|2010-01-01 00:01:00|     1556|   2972|$-77.00| Swipe Transaction|      59935|       Beulah|            ND|58523.0|5499|  NULL|\n",
      "|7475328|2010-01-01 00:02:00|      561|   4575| $14.57| Swipe Transaction|      67570|   Bettendorf|            IA|52722.0|5311|  NULL|\n",
      "|7475329|2010-01-01 00:02:00|     1129|    102| $80.00| Swipe Transaction|      27092|        Vista|            CA|92084.0|4829|  NULL|\n",
      "|7475331|2010-01-01 00:05:00|      430|   2860|$200.00| Swipe Transaction|      27092|  Crown Point|            IN|46307.0|4829|  NULL|\n",
      "|7475332|2010-01-01 00:06:00|      848|   3915| $46.41| Swipe Transaction|      13051|      Harwood|            MD|20776.0|5813|  NULL|\n",
      "|7475333|2010-01-01 00:07:00|     1807|    165|  $4.81| Swipe Transaction|      20519|        Bronx|            NY|10464.0|5942|  NULL|\n",
      "|7475334|2010-01-01 00:09:00|     1556|   2972| $77.00| Swipe Transaction|      59935|       Beulah|            ND|58523.0|5499|  NULL|\n",
      "|7475335|2010-01-01 00:14:00|     1684|   2140| $26.46|Online Transaction|      39021|       ONLINE|          NULL|   NULL|4784|  NULL|\n",
      "|7475336|2010-01-01 00:21:00|      335|   5131|$261.58|Online Transaction|      50292|       ONLINE|          NULL|   NULL|7801|  NULL|\n",
      "|7475337|2010-01-01 00:21:00|      351|   1112| $10.74| Swipe Transaction|       3864|     Flushing|            NY|11355.0|5813|  NULL|\n",
      "+-------+-------------------+---------+-------+-------+------------------+-----------+-------------+--------------+-------+----+------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "#Preview Sample Rows\n",
    "transactions_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0546cb3b-164c-4fb3-a83d-88d67c6babea",
   "metadata": {},
   "source": [
    "#### Check Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f083240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- client_id: integer (nullable = true)\n",
      " |-- card_id: integer (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      " |-- use_chip: string (nullable = true)\n",
      " |-- merchant_id: integer (nullable = true)\n",
      " |-- merchant_city: string (nullable = true)\n",
      " |-- merchant_state: string (nullable = true)\n",
      " |-- zip: double (nullable = true)\n",
      " |-- mcc: integer (nullable = true)\n",
      " |-- errors: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06abda82-4718-4332-94cd-7f033e04db7c",
   "metadata": {},
   "source": [
    "All the datatypes are consistent with the columns, except for the amount feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86c7a62c-6af3-4348-a156-bdb1ea0480c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|amount |\n",
      "+-------+\n",
      "|$-77.00|\n",
      "|$14.57 |\n",
      "|$80.00 |\n",
      "|$200.00|\n",
      "|$46.41 |\n",
      "|$4.81  |\n",
      "|$77.00 |\n",
      "|$26.46 |\n",
      "|$261.58|\n",
      "|$10.74 |\n",
      "+-------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "#Transaction Amount Format Check\n",
    "transactions_df.select(\"amount\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602c351b-ba17-4a10-80bf-48e1ba18f4a2",
   "metadata": {},
   "source": [
    "As this column has a currency sign in the data, it must me removed and converted to float values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01929495-b0f7-4a06-ac66-7e72df385371",
   "metadata": {},
   "source": [
    "#### Check Duplicate Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd41616d-f318-476e-aa95-133f0e3cb60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "duplicate_id_count = (\n",
    "    transactions_df\n",
    "    .groupBy(\"id\")\n",
    "    .agg(count(\"*\").alias(\"cnt\"))\n",
    "    .filter(\"cnt > 1\")\n",
    "    .count()\n",
    ")\n",
    "\n",
    "duplicate_id_count\n",
    "# Number of duplicate ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afdbba9-da0e-45db-8642-a0f4129cb4a5",
   "metadata": {},
   "source": [
    "There are no duplicate transaction ids in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b11a2a-f6f6-4739-909e-1d1cf10e5247",
   "metadata": {},
   "source": [
    "#### Categories and Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "791c68ed-f738-4817-acc0-d567b91e31d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+\n",
      "|          use_chip|  count|\n",
      "+------------------+-------+\n",
      "|Online Transaction|1557912|\n",
      "| Swipe Transaction|6967185|\n",
      "|  Chip Transaction|4780818|\n",
      "+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.groupBy(\"use_chip\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867707d1-5cdb-4871-951b-2408f8b1c7c3",
   "metadata": {},
   "source": [
    "The majority of transactions are **Swipe Transactions**, followed by **Chip Transactions**, while **Online Transactions** represent a smaller portion of the total volume.\n",
    "\n",
    "This distribution is expected in real-world card usage, where most everyday purchases are made using physical cards.  \n",
    "However, even though online transactions are fewer in number, they are often associated with a higher fraud risk due to the absence of physical card verification.\n",
    "\n",
    "For this reason, the transaction type (`use_chip`) is considered an important variable for later analysis and modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba9b4893-4f6d-478f-9189-abf3e7fa5eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|      merchant_state|  count|\n",
      "+--------------------+-------+\n",
      "|                  NJ| 322227|\n",
      "|                  IL| 467930|\n",
      "|United Arab Emirates|    300|\n",
      "|        South Africa|    339|\n",
      "|           Indonesia|    194|\n",
      "|         South Korea|   1153|\n",
      "|              Israel|    941|\n",
      "|           Australia|    569|\n",
      "|              Brunei|      3|\n",
      "|                  CA|1427087|\n",
      "|                  IN| 312468|\n",
      "|                  OK| 159902|\n",
      "|                  KY| 170013|\n",
      "|                  LA| 159719|\n",
      "|                  KS|  99442|\n",
      "|                  WY|   8747|\n",
      "|              Tuvalu|      5|\n",
      "|            Thailand|    461|\n",
      "|             Romania|     50|\n",
      "|    Marshall Islands|     11|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "transactions_df.groupBy(\"merchant_state\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2d5c527-e9f1-4372-95b1-12e55ec25e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|              errors|   count|\n",
      "+--------------------+--------+\n",
      "|    Technical Glitch|   26271|\n",
      "|Bad PIN,Insuffici...|     293|\n",
      "|Bad Card Number,B...|      38|\n",
      "|             Bad CVV|    6106|\n",
      "|Bad Card Number,B...|      33|\n",
      "|Bad PIN,Technical...|      70|\n",
      "|                NULL|13094522|\n",
      "|         Bad Zipcode|    1126|\n",
      "|Insufficient Bala...|     243|\n",
      "|Bad Expiration,In...|      47|\n",
      "|Bad CVV,Insuffici...|      57|\n",
      "|Bad Card Number,T...|      15|\n",
      "|Insufficient Balance|  130902|\n",
      "|Bad Expiration,Ba...|      32|\n",
      "|      Bad Expiration|    6161|\n",
      "|     Bad Card Number|    7767|\n",
      "|             Bad PIN|   32119|\n",
      "|Bad Expiration,Te...|      21|\n",
      "|Bad Card Number,I...|      71|\n",
      "|Bad CVV,Technical...|       8|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "transactions_df.groupBy(\"errors\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d3f22d-8635-4cd2-a38e-f602a076798a",
   "metadata": {},
   "source": [
    "The `errors` column contains information about transactions that encountered issues during processing.\n",
    "Most transactions have a `NULL` value, meaning they were completed successfully without any reported problem.\n",
    "\n",
    "However, a small subset of transactions contains specific error types, such as:\n",
    "- Insufficient balance\n",
    "- Bad PIN\n",
    "- Bad card number\n",
    "- Bad expiration date\n",
    "- Technical glitches\n",
    "\n",
    "Among these, errors related to **insufficient balance**, **bad PIN**, and **technical issues** appear more frequently than others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff88aee-567d-4cbe-a7c6-322ecf42ed66",
   "metadata": {},
   "source": [
    "### 2. Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47517194-3a51-4b88-b77b-47ae16b3f883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in transactions dataset:\n",
      "+---+----+---------+-------+------+--------+-----------+-------------+--------------+-------+---+--------+\n",
      "| id|date|client_id|card_id|amount|use_chip|merchant_id|merchant_city|merchant_state|    zip|mcc|  errors|\n",
      "+---+----+---------+-------+------+--------+-----------+-------------+--------------+-------+---+--------+\n",
      "|  0|   0|        0|      0|     0|       0|          0|            0|       1563726|1652706|  0|13094522|\n",
      "+---+----+---------+-------+------+--------+-----------+-------------+--------------+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check missing values for all the dataset\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "def count_missing_values(df):\n",
    "    missing_counts = df.select([spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "    return missing_counts\n",
    "print(\"Missing values in transactions dataset:\")\n",
    "count_missing_values(transactions_df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab571db0-354e-47fc-839a-6dd7f009d3df",
   "metadata": {},
   "source": [
    "The missing data is in `merchant_state`, `zip`, and `errors`. Most of the `merchant_state` and `zip` is from the online transactions. So, these data can be filled with that clearly states that it is an online transaction to preseve the semantic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f54254-b474-4613-aedf-41bffb784564",
   "metadata": {},
   "source": [
    "### 3. Standardizing Categorical Variables\n",
    "\n",
    "As we saw the data, the categorical columns have formatted data already. There is no further formatting required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6a87b7-aac4-4547-9925-cbcb6c1da9a9",
   "metadata": {},
   "source": [
    "### 4. Data Integrity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08aa6c3-4764-49cb-954e-814f32e2e412",
   "metadata": {},
   "source": [
    "#### Transactions Consistency\n",
    "\n",
    "We are checking if the merchant_id maps consistently to the same merchant location exceot for online transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c752453d-43d7-4024-8a3b-3b283718e9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|merchant_id|num_locations|\n",
      "+-----------+-------------+\n",
      "|54850      |588          |\n",
      "|27092      |1049         |\n",
      "|11468      |368          |\n",
      "|9041       |7            |\n",
      "|44919      |744          |\n",
      "|28395      |398          |\n",
      "|86438      |1305         |\n",
      "|31258      |2            |\n",
      "|36392      |2            |\n",
      "|18014      |5            |\n",
      "|7131       |2            |\n",
      "|58897      |5            |\n",
      "|95855      |112          |\n",
      "|83240      |3            |\n",
      "|57386      |85           |\n",
      "|78680      |2            |\n",
      "|24891      |9            |\n",
      "|11901      |29           |\n",
      "|94625      |256          |\n",
      "|30928      |217          |\n",
      "+-----------+-------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Filter non-online transactions\n",
    "physical_txns = transactions_df.filter(\n",
    "    F.col(\"use_chip\") != \"Online Transaction\"\n",
    ")\n",
    "\n",
    "# Count distinct city/state combinations per merchant\n",
    "merchant_location_check = (\n",
    "    physical_txns\n",
    "    .groupBy(\"merchant_id\")\n",
    "    .agg(\n",
    "        F.countDistinct(\n",
    "            F.struct(\"merchant_city\", \"merchant_state\")\n",
    "        ).alias(\"num_locations\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Merchants with inconsistent locations\n",
    "inconsistent_merchants = merchant_location_check.filter(\n",
    "    F.col(\"num_locations\") > 1\n",
    ")\n",
    "\n",
    "inconsistent_merchants.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c614a5-a975-44cc-b2b7-61068331b8e7",
   "metadata": {},
   "source": [
    "These merchants have multiple locations, which may indicate either: \n",
    "- Data quality issue\n",
    "- Chain merchants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52c4e9b-b383-4780-a57a-133c8feaf44d",
   "metadata": {},
   "source": [
    "We are also checking if the online transaction data is having any physical location to check the integrity of the transaction details. \n",
    "\n",
    "For that we are using a Filter use_chip == 'Online Transaction'\n",
    "and checking if merchant_state, or zip are filled\n",
    "\n",
    "Any non-null values will be an integrity error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7346d91-573c-410d-9447-5207a32846c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+-------+------+--------+-----------+-------------+--------------+---+---+------+\n",
      "|id |date|client_id|card_id|amount|use_chip|merchant_id|merchant_city|merchant_state|zip|mcc|errors|\n",
      "+---+----+---------+-------+------+--------+-----------+-------------+--------------+---+---+------+\n",
      "+---+----+---------+-------+------+--------+-----------+-------------+--------------+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "online_location_violations = transactions_df.filter(\n",
    "    (F.col(\"use_chip\") == \"Online Transaction\") &\n",
    "    (\n",
    "        (F.col(\"merchant_city\").isNotNull() & (F.col(\"merchant_city\") != \"ONLINE\")) |\n",
    "        (F.col(\"merchant_state\").isNotNull()) |\n",
    "        (F.col(\"zip\").isNotNull())\n",
    "    )\n",
    ")\n",
    "\n",
    "online_location_violations.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee69f7dc-5a85-499b-a9dd-4a8109424052",
   "metadata": {},
   "source": [
    "As we see there are no fraudulent data for the online transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1f6aee-f085-4583-943b-efcc9cb29862",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Based on the Analyis, the dataset is cleaned and exported for our further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "305104e1-684a-4cd3-89c5-43544524bb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transactions data after cleaning\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- client_id: integer (nullable = true)\n",
      " |-- card_id: integer (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- use_chip: string (nullable = true)\n",
      " |-- merchant_id: integer (nullable = true)\n",
      " |-- merchant_city: string (nullable = true)\n",
      " |-- merchant_state: string (nullable = true)\n",
      " |-- zip: double (nullable = true)\n",
      " |-- mcc: integer (nullable = true)\n",
      " |-- errors: string (nullable = true)\n",
      "\n",
      "+-------+-------------------+---------+-------+------+------------------+-----------+-------------+--------------+-------+----+--------+\n",
      "|     id|               date|client_id|card_id|amount|          use_chip|merchant_id|merchant_city|merchant_state|    zip| mcc|  errors|\n",
      "+-------+-------------------+---------+-------+------+------------------+-----------+-------------+--------------+-------+----+--------+\n",
      "|7475327|2010-01-01 00:01:00|     1556|   2972| -77.0| Swipe Transaction|      59935|       BEULAH|            ND|58523.0|5499|No Error|\n",
      "|7475328|2010-01-01 00:02:00|      561|   4575| 14.57| Swipe Transaction|      67570|   BETTENDORF|            IA|52722.0|5311|No Error|\n",
      "|7475329|2010-01-01 00:02:00|     1129|    102|  80.0| Swipe Transaction|      27092|        VISTA|            CA|92084.0|4829|No Error|\n",
      "|7475331|2010-01-01 00:05:00|      430|   2860| 200.0| Swipe Transaction|      27092|  CROWN POINT|            IN|46307.0|4829|No Error|\n",
      "|7475332|2010-01-01 00:06:00|      848|   3915| 46.41| Swipe Transaction|      13051|      HARWOOD|            MD|20776.0|5813|No Error|\n",
      "|7475333|2010-01-01 00:07:00|     1807|    165|  4.81| Swipe Transaction|      20519|        BRONX|            NY|10464.0|5942|No Error|\n",
      "|7475334|2010-01-01 00:09:00|     1556|   2972|  77.0| Swipe Transaction|      59935|       BEULAH|            ND|58523.0|5499|No Error|\n",
      "|7475335|2010-01-01 00:14:00|     1684|   2140| 26.46|Online Transaction|      39021|       ONLINE|          NULL|   NULL|4784|No Error|\n",
      "|7475336|2010-01-01 00:21:00|      335|   5131|261.58|Online Transaction|      50292|       ONLINE|          NULL|   NULL|7801|No Error|\n",
      "|7475337|2010-01-01 00:21:00|      351|   1112| 10.74| Swipe Transaction|       3864|     FLUSHING|            NY|11355.0|5813|No Error|\n",
      "+-------+-------------------+---------+-------+------+------------------+-----------+-------------+--------------+-------+----+--------+\n",
      "only showing top 10 rows\n",
      "Merchant data location\n",
      "+-----------+----------+-----------+--------------------+\n",
      "|merchant_id|city_count|state_count|       all_locations|\n",
      "+-----------+----------+-----------+--------------------+\n",
      "|         70|         2|          2|PHILADELPHIA PA |...|\n",
      "|         74|         2|          2|WALDORF MD | LOUI...|\n",
      "|        146|         3|          3|POMPANO BEACH FL ...|\n",
      "|        157|         2|          2|YORK NE | POUGHKE...|\n",
      "|        186|         3|          3|FORT WORTH TX | B...|\n",
      "+-----------+----------+-----------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, when, regexp_replace, countDistinct, upper, trim, collect_set, concat_ws, concat, lit\n",
    ")\n",
    "\n",
    "\n",
    "# 1. Clean amount\n",
    "transactions_clean = transactions_df.withColumn(\n",
    "    \"amount\",\n",
    "    regexp_replace(col(\"amount\"), \"[$,]\", \"\")\n",
    ")\n",
    "\n",
    "transactions_clean = transactions_clean.withColumn(\n",
    "    \"amount\",\n",
    "    col(\"amount\").cast(\"double\")\n",
    ")\n",
    "\n",
    "# 2. Handling missing errors\n",
    "transactions_clean = transactions_clean.withColumn(\n",
    "    \"errors\",\n",
    "    when(col(\"errors\").isNull(), \"No Error\").otherwise(col(\"errors\"))\n",
    ")\n",
    "\n",
    "# 3. Normalize merchant location\n",
    "transactions_clean = transactions_clean.withColumn(\n",
    "    \"merchant_city\",\n",
    "    upper(trim(col(\"merchant_city\")))\n",
    ").withColumn(\n",
    "    \"merchant_state\",\n",
    "    upper(trim(col(\"merchant_state\")))\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 4. Merchant location consistency check\n",
    "\n",
    "merchant_location_check = (\n",
    "    transactions_clean\n",
    "    .filter(col(\"use_chip\") != \"Online Transaction\")\n",
    "    .groupBy(\"merchant_id\")\n",
    "    .agg(\n",
    "        countDistinct(\"merchant_city\").alias(\"city_count\"),\n",
    "        countDistinct(\"merchant_state\").alias(\"state_count\"),\n",
    "        concat_ws(\n",
    "            \" | \",\n",
    "            collect_set(\n",
    "                concat(col(\"merchant_city\"), lit(\" \"), col(\"merchant_state\"))\n",
    "            )\n",
    "        ).alias(\"all_locations\")\n",
    "    )\n",
    "    .filter((col(\"city_count\") > 1) | (col(\"state_count\") > 1))\n",
    ")\n",
    "\n",
    "\n",
    "# Final verification\n",
    "print(\"Transactions data after cleaning\")\n",
    "\n",
    "transactions_clean.printSchema()\n",
    "\n",
    "transactions_clean.show(10)\n",
    "\n",
    "\n",
    "print(\"Merchant data location\")\n",
    "merchant_location_check.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a1956e-a8e6-4f67-8a01-1ec31e62f2ec",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23791a4d-dfcb-4536-aa99-8a84cf978607",
   "metadata": {},
   "source": [
    "Feature engineering transforms cleaned raw transaction data into meaningful variables that help capture spending behavior, transaction patterns, and potential fraud indicators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f264bdfd-b2d4-42e2-8dbc-2145f2f029c5",
   "metadata": {},
   "source": [
    "#### Time-based Features\n",
    "\n",
    "Transaction timing is a strong behavioral indicator. Fraudulent transactions often occur at unusual hours (late night / early morning).\n",
    "\n",
    "Steps:\n",
    "- Extract the hour from the date timestamp.\n",
    "- Categorize transactions into time-of-day buckets.\n",
    "- Time Buckets for morning, afternoon, evening and night\n",
    "\n",
    "Engineered Feature `day_timing`: categorical feature representing time of day\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "813b5ec3-6c0a-4718-8273-25e9242e4b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, hour, when\n",
    ")\n",
    "\n",
    "transactions_fe = transactions_clean.withColumn(\n",
    "    \"transaction_hour\",\n",
    "    hour(col(\"date\"))\n",
    ")\n",
    "\n",
    "transactions_fe = transactions_fe.withColumn(\n",
    "    \"day_timing\",\n",
    "    when((col(\"transaction_hour\") >= 5) & (col(\"transaction_hour\") < 12), \"Morning\")\n",
    "    .when((col(\"transaction_hour\") >= 12) & (col(\"transaction_hour\") < 17), \"Afternoon\")\n",
    "    .when((col(\"transaction_hour\") >= 17) & (col(\"transaction_hour\") < 21), \"Evening\")\n",
    "    .otherwise(\"Night\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04b4fd6-3c70-4a4b-a00f-8ea9dd097865",
   "metadata": {},
   "source": [
    "#### Behavioral Data Flags\n",
    "\n",
    "Binary flags simplify model learning and improve interpretability.\n",
    "\n",
    "##### Online Transaction Flag\n",
    "\n",
    "    Online transactions are different from swipe transactions. It Helps isolate e-commerce fraud patterns.\n",
    "\n",
    "Feature `is_online_transaction` : \n",
    "    \n",
    "        1 → Online Transaction\n",
    "        0 → Other transaction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "606396b8-12dd-4a05-a9d8-434faeb0919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online transaction flag\n",
    "transactions_fe = transactions_fe.withColumn(\n",
    "    \"is_online_transaction\",\n",
    "    when(col(\"use_chip\") == \"Online Transaction\", 1).otherwise(0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b235ad-abfd-43c2-acff-c2197f532e51",
   "metadata": {},
   "source": [
    "#### Refund Detection Flag\n",
    "\n",
    "    Negative transaction amounts represent refunds. Refunds have different risk characteristics.\n",
    "\n",
    "Feature `is_refund` :\n",
    "\n",
    "        1 → amount < 0\n",
    "        0 → amount ≥ 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d05f14cc-0a6b-4a4a-8d7b-ec89e75bcd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refund flag\n",
    "transactions_fe = transactions_fe.withColumn(\n",
    "    \"is_refund\",\n",
    "    when(col(\"amount\") < 0, 1).otherwise(0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a59eccc-d3b1-47e4-a161-24ed189c1f36",
   "metadata": {},
   "source": [
    "#### Error Flag\n",
    "\n",
    "    Original errors column contains many categories. Binary flag simplifies analysis.\n",
    "\n",
    "Feature `has_error` :\n",
    "\n",
    "        1 → error present\n",
    "        0 → no error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84734ca5-e33a-415a-92aa-2ba79dc3f753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error presence flag\n",
    "transactions_fe = transactions_fe.withColumn(\n",
    "    \"has_error\",\n",
    "    when(col(\"errors\") != \"No Error\", 1).otherwise(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26cbe615-8d66-418b-9537-08aa34871e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- client_id: integer (nullable = true)\n",
      " |-- card_id: integer (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- use_chip: string (nullable = true)\n",
      " |-- merchant_id: integer (nullable = true)\n",
      " |-- merchant_city: string (nullable = true)\n",
      " |-- merchant_state: string (nullable = true)\n",
      " |-- zip: double (nullable = true)\n",
      " |-- mcc: integer (nullable = true)\n",
      " |-- errors: string (nullable = true)\n",
      " |-- transaction_hour: integer (nullable = true)\n",
      " |-- day_timing: string (nullable = false)\n",
      " |-- is_online_transaction: integer (nullable = false)\n",
      " |-- is_refund: integer (nullable = false)\n",
      " |-- has_error: integer (nullable = false)\n",
      "\n",
      "+-------+-------------------+---------+-------+------+------------------+-----------+-------------+--------------+-------+----+--------+----------------+----------+---------------------+---------+---------+\n",
      "|id     |date               |client_id|card_id|amount|use_chip          |merchant_id|merchant_city|merchant_state|zip    |mcc |errors  |transaction_hour|day_timing|is_online_transaction|is_refund|has_error|\n",
      "+-------+-------------------+---------+-------+------+------------------+-----------+-------------+--------------+-------+----+--------+----------------+----------+---------------------+---------+---------+\n",
      "|7475327|2010-01-01 00:01:00|1556     |2972   |-77.0 |Swipe Transaction |59935      |BEULAH       |ND            |58523.0|5499|No Error|0               |Night     |0                    |1        |0        |\n",
      "|7475328|2010-01-01 00:02:00|561      |4575   |14.57 |Swipe Transaction |67570      |BETTENDORF   |IA            |52722.0|5311|No Error|0               |Night     |0                    |0        |0        |\n",
      "|7475329|2010-01-01 00:02:00|1129     |102    |80.0  |Swipe Transaction |27092      |VISTA        |CA            |92084.0|4829|No Error|0               |Night     |0                    |0        |0        |\n",
      "|7475331|2010-01-01 00:05:00|430      |2860   |200.0 |Swipe Transaction |27092      |CROWN POINT  |IN            |46307.0|4829|No Error|0               |Night     |0                    |0        |0        |\n",
      "|7475332|2010-01-01 00:06:00|848      |3915   |46.41 |Swipe Transaction |13051      |HARWOOD      |MD            |20776.0|5813|No Error|0               |Night     |0                    |0        |0        |\n",
      "|7475333|2010-01-01 00:07:00|1807     |165    |4.81  |Swipe Transaction |20519      |BRONX        |NY            |10464.0|5942|No Error|0               |Night     |0                    |0        |0        |\n",
      "|7475334|2010-01-01 00:09:00|1556     |2972   |77.0  |Swipe Transaction |59935      |BEULAH       |ND            |58523.0|5499|No Error|0               |Night     |0                    |0        |0        |\n",
      "|7475335|2010-01-01 00:14:00|1684     |2140   |26.46 |Online Transaction|39021      |ONLINE       |NULL          |NULL   |4784|No Error|0               |Night     |1                    |0        |0        |\n",
      "|7475336|2010-01-01 00:21:00|335      |5131   |261.58|Online Transaction|50292      |ONLINE       |NULL          |NULL   |7801|No Error|0               |Night     |1                    |0        |0        |\n",
      "|7475337|2010-01-01 00:21:00|351      |1112   |10.74 |Swipe Transaction |3864       |FLUSHING     |NY            |11355.0|5813|No Error|0               |Night     |0                    |0        |0        |\n",
      "+-------+-------------------+---------+-------+------+------------------+-----------+-------------+--------------+-------+----+--------+----------------+----------+---------------------+---------+---------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "transactions_fe.printSchema()\n",
    "transactions_fe.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47481fb7-3217-46d1-ac30-8b11f32e6c86",
   "metadata": {},
   "source": [
    "## Cards Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26484d3-9ac3-43ab-a619-4ffe1a8863fb",
   "metadata": {},
   "source": [
    "Financial institutions issue multiple payment cards to users over time.\n",
    "This dataset captures card-level attributes, including card brand, type, issuance history, security properties, and risk indicators such as dark web exposure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c47de6f-db2b-4a99-9475-1fdbc56b1b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'client_id',\n",
       " 'card_brand',\n",
       " 'card_type',\n",
       " 'card_number',\n",
       " 'expires',\n",
       " 'cvv',\n",
       " 'has_chip',\n",
       " 'num_cards_issued',\n",
       " 'credit_limit',\n",
       " 'acct_open_date',\n",
       " 'year_pin_last_changed',\n",
       " 'card_on_dark_web']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cards_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec704ed-e8d4-4b34-adeb-01b07fdbb82a",
   "metadata": {},
   "source": [
    "### 1. Data Inspection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f43d362-b3cf-4279-8336-72daba5fa307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+----------+---------------+----------------+-------+---+--------+----------------+------------+--------------+---------------------+----------------+\n",
      "|  id|client_id|card_brand|      card_type|     card_number|expires|cvv|has_chip|num_cards_issued|credit_limit|acct_open_date|year_pin_last_changed|card_on_dark_web|\n",
      "+----+---------+----------+---------------+----------------+-------+---+--------+----------------+------------+--------------+---------------------+----------------+\n",
      "|4524|      825|      NULL|          Debit|4344676511950444|12/2022|623|     YES|               2|      $24295|       09/2002|                 2008|              No|\n",
      "|2731|      825|      NULL|          Debit|4956965974959986|12/2020|393|     YES|               2|      $21968|       04/2014|                 2014|              No|\n",
      "|3701|      825|      Visa|          Debit|4582313478255491|02/2024|719|     YES|               2|      $46414|       07/2003|                 2004|              No|\n",
      "|  42|      825|      NULL|         Credit|4879494103069057|08/2024|693|      NO|               1|      $12400|       01/2003|                 2012|              No|\n",
      "|4659|      825|      NULL|Debit (Prepaid)|5722874738736011|03/2009| 75|     YES|               1|         $28|       09/2008|                 2009|              No|\n",
      "|4537|     1746|      Visa|         Credit|4404898874682993|09/2003|736|     YES|               1|      $27500|       09/2003|                 2012|              No|\n",
      "|1278|     1746|      Visa|          Debit|4001482973848631|07/2022|972|     YES|               2|      $28508|       02/2011|                 2011|              No|\n",
      "|3687|     1746|Mastercard|          Debit|5627220683410948|06/2022| 48|     YES|               2|       $9022|       07/2003|                 2015|              No|\n",
      "|3465|     1746|      NULL|Debit (Prepaid)|5711382187309326|11/2020|722|     YES|               2|         $54|       06/2010|                 2015|              No|\n",
      "|3754|     1746|Mastercard|Debit (Prepaid)|5766121508358701|02/2023|908|     YES|               1|         $99|       07/2006|                 2012|              No|\n",
      "+----+---------+----------+---------------+----------------+-------+---+--------+----------------+------------+--------------+---------------------+----------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "cards_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d72e06-3c22-4a1a-bc8f-2cb8348e0610",
   "metadata": {},
   "source": [
    "#### Check Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78d9d97c-7ddf-4b44-adb1-81e161600207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- client_id: integer (nullable = true)\n",
      " |-- card_brand: string (nullable = true)\n",
      " |-- card_type: string (nullable = true)\n",
      " |-- card_number: long (nullable = true)\n",
      " |-- expires: string (nullable = true)\n",
      " |-- cvv: integer (nullable = true)\n",
      " |-- has_chip: string (nullable = true)\n",
      " |-- num_cards_issued: integer (nullable = true)\n",
      " |-- credit_limit: string (nullable = true)\n",
      " |-- acct_open_date: string (nullable = true)\n",
      " |-- year_pin_last_changed: integer (nullable = true)\n",
      " |-- card_on_dark_web: string (nullable = true)\n",
      "\n",
      "Total number of rows:  6146\n"
     ]
    }
   ],
   "source": [
    "cards_df.printSchema()\n",
    "\n",
    "print(\"Total number of rows: \", cards_df.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29747a5d-b6af-4c4c-8b03-756c4a76619e",
   "metadata": {},
   "source": [
    "- Date-related fields (expires, acct_open_date) are currently stored as string and will need conversion to date formats during cleaning.\n",
    "- Columns such as has_chip, and card_on_dark_web are incorrectly stored as string types\n",
    "- The credit_limit column is stored as string, as it contains currency symbols and must be converted to numeric/float."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db21ac6-295a-4ec8-a08b-eb501cf234d6",
   "metadata": {},
   "source": [
    "#### Check Duplicate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44e98261-6512-4069-bed4-deecdb61bb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|count|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "duplicate_cards = cards_df.groupBy(\"id\").count().filter(col(\"count\") > 1)\n",
    "duplicate_cards.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c469969-b5aa-4adc-802b-dc240e76d37f",
   "metadata": {},
   "source": [
    "No duplicate rows were detected in the Cards dataset based on the id column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fa0fc4-35bf-45b9-9606-05c68cdea2eb",
   "metadata": {},
   "source": [
    "#### Category and Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5647da31-5bb5-4c7f-9519-dbc6a5832414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for column: card_brand\n",
      "+----------+-----+\n",
      "|card_brand|count|\n",
      "+----------+-----+\n",
      "|Mastercard| 3150|\n",
      "|      Visa| 2292|\n",
      "|      Amex|  402|\n",
      "|  Discover|  209|\n",
      "|      NULL|   93|\n",
      "+----------+-----+\n",
      "\n",
      "Value counts for column: card_type\n",
      "+---------------+-----+\n",
      "|      card_type|count|\n",
      "+---------------+-----+\n",
      "|          Debit| 3485|\n",
      "|         Credit| 2044|\n",
      "|Debit (Prepaid)|  574|\n",
      "|            Det|    4|\n",
      "|           Dbit|    4|\n",
      "|            Dit|    4|\n",
      "|           Cret|    3|\n",
      "|           Cdit|    3|\n",
      "| Dbit (Prepaid)|    2|\n",
      "|           Deit|    2|\n",
      "|            bit|    2|\n",
      "|        Credoit|    1|\n",
      "|       Crenodit|    1|\n",
      "|         De=bit|    1|\n",
      "|         Deblit|    1|\n",
      "|           Crit|    1|\n",
      "|           Debt|    1|\n",
      "|          Crdit|    1|\n",
      "|         Debbit|    1|\n",
      "|       Cr iedit|    1|\n",
      "+---------------+-----+\n",
      "only showing top 20 rows\n",
      "Value counts for column: has_chip\n",
      "+--------+-----+\n",
      "|has_chip|count|\n",
      "+--------+-----+\n",
      "|     YES| 5500|\n",
      "|      NO|  646|\n",
      "+--------+-----+\n",
      "\n",
      "Value counts for column: card_on_dark_web\n",
      "+----------------+-----+\n",
      "|card_on_dark_web|count|\n",
      "+----------------+-----+\n",
      "|              No| 6146|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "# List of categorical columns\n",
    "categorical_cols = [\"card_brand\", \"card_type\", \"has_chip\", \"card_on_dark_web\"]\n",
    "\n",
    "# Show value counts for each categorical column\n",
    "for col_name in categorical_cols:\n",
    "    print(f\"Value counts for column: {col_name}\")\n",
    "    cards_df.groupBy(col_name).count().orderBy(col(\"count\").desc()).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599a8a6f-3f02-415f-8d25-fbf5517100fa",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "    1. card_brand:\n",
    "\n",
    "        Major brands: Mastercard (3150), Visa (2292), Amex (402), Discover (209)\n",
    "        Missing values: 93\n",
    "        card_brand missing values should be filled as \"Unknown\".\n",
    "\n",
    "    2. card_type:\n",
    "\n",
    "        Debit (3485), Credit (2044), and Debit (Prepaid) (574)\n",
    "        Several inconsistent or misspelled entries: Det, Dbit, Dit, Cret, Cdit, etc.\n",
    "        card_type contains multiple misspellings and variations; standardization is required\n",
    "\n",
    "    3. has_chip:\n",
    "\n",
    "        YES: 5500, NO: 646\n",
    "        No missing values\n",
    "\n",
    "    4. card_on_dark_web:\n",
    "    \n",
    "        All values are \"No\", no variation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce903b42-392a-4809-abd2-a999639968a8",
   "metadata": {},
   "source": [
    "### 2. Checking Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ab3a1a8-9350-47a9-b7d3-f7ac2b00636e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+---------+-----------+-------+---+--------+----------------+------------+--------------+---------------------+----------------+------------------+\n",
      "|id |client_id|card_brand|card_type|card_number|expires|cvv|has_chip|num_cards_issued|credit_limit|acct_open_date|year_pin_last_changed|card_on_dark_web|credit_limit_clean|\n",
      "+---+---------+----------+---------+-----------+-------+---+--------+----------------+------------+--------------+---------------------+----------------+------------------+\n",
      "|0  |0        |93        |0        |0          |0      |0  |0       |0               |0           |0             |0                    |0               |0                 |\n",
      "+---+---------+----------+---------+-----------+-------+---+--------+----------------+------------+--------------+---------------------+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "missing_summary = cards_df.select(\n",
    "    *[\n",
    "        spark_sum(col(c).isNull().cast(\"int\")).alias(c)\n",
    "        for c in cards_df.columns\n",
    "    ]\n",
    ")\n",
    "\n",
    "missing_summary.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4311c9-e62e-4b16-8ebf-e6f7e4490e74",
   "metadata": {},
   "source": [
    "The column card_brand has 93 missing values out of 6146 rows. Other columns appear to have complete values.\n",
    "\n",
    "- Missing card_brand likely indicates legacy or unbranded cards.\\\n",
    "- Dropping these rows would result in data loss; instead, these missing values should be handled carefully during cleaning.\n",
    "\n",
    "Impute missing card_brand values with \"Unknown\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b81ee2c-e51e-4bcf-8666-607c8ed1df61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------------+\n",
      "|missing_credit_limit|missing_num_cards_issued|\n",
      "+--------------------+------------------------+\n",
      "|                   0|                       0|\n",
      "+--------------------+------------------------+\n",
      "\n",
      "+-------+------------------+------------------+\n",
      "|summary|      credit_limit|  num_cards_issued|\n",
      "+-------+------------------+------------------+\n",
      "|  count|              6146|              6146|\n",
      "|   mean|14347.493979824276|1.5030914415880248|\n",
      "| stddev|12014.463884038893|0.5191909056590772|\n",
      "|    min|               0.0|                 1|\n",
      "|    max|          151223.0|                 3|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, count, when\n",
    "\n",
    "# Clean credit_limit temporarily to check numeric conversion\n",
    "cards_df = cards_df.withColumn(\n",
    "    \"credit_limit_clean\",\n",
    "    regexp_replace(col(\"credit_limit\"), \"[$,]\", \"\")\n",
    ")\n",
    "\n",
    "# Count missing or null values\n",
    "cards_df.select(\n",
    "    count(when(col(\"credit_limit_clean\").isNull() | (col(\"credit_limit_clean\") == \"\"), True)).alias(\"missing_credit_limit\"),\n",
    "    count(when(col(\"num_cards_issued\").isNull(), True)).alias(\"missing_num_cards_issued\")\n",
    ").show()\n",
    "\n",
    "# Show min and max for numeric check\n",
    "cards_df.select(\n",
    "    col(\"credit_limit_clean\").cast(\"double\").alias(\"credit_limit\"),\n",
    "    col(\"num_cards_issued\")\n",
    ").describe().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35075cdf-5f8e-4c23-99f4-53a1bde4a705",
   "metadata": {},
   "source": [
    "- credit_limit:\n",
    "        No missing values.\n",
    "        Values range from 0 to 151,223, with a mean of ~14,347 and standard deviation of ~12,014.\n",
    "        A few cards have a credit limit of 0, likely corresponding to debit or prepaid cards.\n",
    "\n",
    "- num_cards_issued:\n",
    "        No missing values.\n",
    "        Values range from 1 to 3, with a mean of ~1.5, indicating most clients hold 1–2 cards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bc2207-3b25-4479-bbe2-e12abdbf2529",
   "metadata": {},
   "source": [
    "### 3. Data Integrity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d1c870-3103-4afa-9cc3-337067de9ea7",
   "metadata": {},
   "source": [
    "#### Cards with zero or negative credit limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de174539-7237-4add-8da9-804903cd79bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---------+------------------+\n",
      "|  id|client_id|card_type|credit_limit_clean|\n",
      "+----+---------+---------+------------------+\n",
      "|4318|      668|   Credit|               0.0|\n",
      "|3626|      870|   Credit|               0.0|\n",
      "|5957|     1975|   Credit|               0.0|\n",
      "|1799|      934|   Credit|               0.0|\n",
      "| 782|     1658|   Credit|               0.0|\n",
      "|  96|       81|   Credit|               0.0|\n",
      "|5680|      214|   Credit|               0.0|\n",
      "|3443|      846|   Credit|               0.0|\n",
      "|5264|     1770|   Credit|               0.0|\n",
      "|3182|       15|   Credit|               0.0|\n",
      "|3183|      318|   Credit|               0.0|\n",
      "|6081|     1260|   Credit|               0.0|\n",
      "|2105|      649|   Credit|               0.0|\n",
      "|2411|     1364|   Credit|               0.0|\n",
      "|2707|      132|   Credit|               0.0|\n",
      "|4610|     1306|   Credit|               0.0|\n",
      "| 265|       37|   Credit|               0.0|\n",
      "|2885|     1202|   Credit|               0.0|\n",
      "| 905|     1224|   Credit|               0.0|\n",
      "|3086|     1348|   Credit|               0.0|\n",
      "+----+---------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "Credit cards with limit 0: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace\n",
    "\n",
    "cards_clean = cards_df.withColumn(\n",
    "    \"credit_limit_clean\",\n",
    "    regexp_replace(col(\"credit_limit\"), \"[$,]\", \"\").cast(\"double\")\n",
    ")\n",
    "cards_clean.filter(\n",
    "    (col(\"card_type\") == \"Credit\") & (col(\"credit_limit_clean\") <= 0)\n",
    ").select(\n",
    "    \"id\", \"client_id\", \"card_type\", \"credit_limit_clean\"\n",
    ").show()\n",
    "\n",
    "\n",
    "print(\"Credit cards with limit 0: \")\n",
    "# Count Credit cards with zero or negative limits\n",
    "cards_clean.filter(\n",
    "    (col(\"card_type\") == \"Credit\") & (col(\"credit_limit_clean\") <= 0)\n",
    ").count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b934c00-282b-4073-abb1-2a539b6ad56c",
   "metadata": {},
   "source": [
    "Count of Credit cards with zero or negative credit limits: 26.\n",
    "\n",
    "A small number of Credit cards have zero limits, which could indicate inactive cards or placeholders. The 26 cards with zero limits should be flagged for review or treated carefully during modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4308588-b39e-48f8-b752-0bb91af8c454",
   "metadata": {},
   "source": [
    "#### Card Expiry vs Account Open Date validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b9fd9bd-f56a-43b0-bec2-54164864ed1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------------+-------+---------+-----------------+\n",
      "|id |client_id|acct_open_date|expires|expiry_dt|acct_open_date_dt|\n",
      "+---+---------+--------------+-------+---------+-----------------+\n",
      "+---+---------+--------------+-------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, when\n",
    "\n",
    "cards_clean = cards_clean.withColumn(\n",
    "    \"expiry_dt\",\n",
    "    when(\n",
    "        col(\"expires\").rlike(r\"^\\d{2}/\\d{2}$\"),  # format MM/yy\n",
    "        to_timestamp(col(\"expires\"), \"MM/yy\")\n",
    "    ).when(\n",
    "        col(\"expires\").rlike(r\"^[A-Za-z]{3}-\\d{2}$\"),  # format MMM-yy\n",
    "        to_timestamp(col(\"expires\"), \"MMM-yy\")\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "# Account open date parsing\n",
    "cards_clean = cards_clean.withColumn(\n",
    "    \"acct_open_date_dt\",\n",
    "    when(\n",
    "        col(\"acct_open_date\").rlike(r\"^[A-Za-z]{3}-\\d{2}$\"),  # format MMM-yy\n",
    "        to_timestamp(col(\"acct_open_date\"), \"MMM-yy\")\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "# Filter cards where expiry < account open date\n",
    "cards_clean.filter(col(\"expiry_dt\") < col(\"acct_open_date_dt\")).select(\n",
    "    \"id\", \"client_id\", \"acct_open_date\", \"expires\", \"expiry_dt\", \"acct_open_date_dt\"\n",
    ").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062aa7ec-3f87-4ff8-9d31-02c261ac539e",
   "metadata": {},
   "source": [
    "- We validated that each card’s expiry date occurs after the account open date:\n",
    "- Converted the expires and acct_open_date columns to proper timestamp format.\n",
    "- Checked for any cards where expiry_dt < acct_open_date_dt.\n",
    "\n",
    " No cards violate this rule; all expiry dates are consistent with account creation dates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14020604-cdc9-4594-9076-05dfea4fc37e",
   "metadata": {},
   "source": [
    "#### Pin Change Year Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76b6564c-44a8-422d-81f6-c6eb45b92e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------------+---------------------+\n",
      "| id|client_id|acct_open_date|year_pin_last_changed|\n",
      "+---+---------+--------------+---------------------+\n",
      "+---+---------+--------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, year, to_date\n",
    "\n",
    "# Extract account open year\n",
    "cards_clean = cards_clean.withColumn(\n",
    "    \"acct_open_year\",\n",
    "    year(to_date(col(\"acct_open_date_dt\")))\n",
    ")\n",
    "\n",
    "# Check for PIN change year validity\n",
    "invalid_pin_years = cards_clean.filter(\n",
    "    (col(\"year_pin_last_changed\") < col(\"acct_open_year\")) |\n",
    "    (col(\"year_pin_last_changed\") > 2025)  # Replace with current year if needed\n",
    ").select(\n",
    "    \"id\", \"client_id\", \"acct_open_date\", \"year_pin_last_changed\"\n",
    ")\n",
    "\n",
    "# Show invalid PIN change year records\n",
    "invalid_pin_years.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef7fb07-b452-4e11-9e74-832f83030ad8",
   "metadata": {},
   "source": [
    "All cards in the dataset have valid PIN change years. Each year_pin_last_changed is greater than or equal to the account open year and less than or equal to the current year. No inconsistencies were detected, indicating that the PIN history is consistent with the account lifecycle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05e4c24-8d9a-4bd3-88c7-ddd0a39d6431",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f3e6e1a-2ba9-4888-86eb-ca1cdda8ed9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+----------+---------------+----------------+-------+---+--------+----------------+------------+--------------+---------------------+----------------+------------------+\n",
      "|id  |client_id|card_brand|card_type      |card_number     |expires|cvv|has_chip|num_cards_issued|credit_limit|acct_open_date|year_pin_last_changed|card_on_dark_web|credit_limit_clean|\n",
      "+----+---------+----------+---------------+----------------+-------+---+--------+----------------+------------+--------------+---------------------+----------------+------------------+\n",
      "|4524|825      |UNKNOWN   |DEBIT          |4344676511950444|12/2022|623|YES     |2               |$24295      |09/2002       |2008                 |NO              |24295.0           |\n",
      "|2731|825      |UNKNOWN   |DEBIT          |4956965974959986|12/2020|393|YES     |2               |$21968      |04/2014       |2014                 |NO              |21968.0           |\n",
      "|3701|825      |VISA      |DEBIT          |4582313478255491|02/2024|719|YES     |2               |$46414      |07/2003       |2004                 |NO              |46414.0           |\n",
      "|42  |825      |UNKNOWN   |CREDIT         |4879494103069057|08/2024|693|NO      |1               |$12400      |01/2003       |2012                 |NO              |12400.0           |\n",
      "|4659|825      |UNKNOWN   |DEBIT (PREPAID)|5722874738736011|03/2009|75 |YES     |1               |$28         |09/2008       |2009                 |NO              |28.0              |\n",
      "|4537|1746     |VISA      |CREDIT         |4404898874682993|09/2003|736|YES     |1               |$27500      |09/2003       |2012                 |NO              |27500.0           |\n",
      "|1278|1746     |VISA      |DEBIT          |4001482973848631|07/2022|972|YES     |2               |$28508      |02/2011       |2011                 |NO              |28508.0           |\n",
      "|3687|1746     |MASTERCARD|DEBIT          |5627220683410948|06/2022|48 |YES     |2               |$9022       |07/2003       |2015                 |NO              |9022.0            |\n",
      "|3465|1746     |UNKNOWN   |DEBIT (PREPAID)|5711382187309326|11/2020|722|YES     |2               |$54         |06/2010       |2015                 |NO              |54.0              |\n",
      "|3754|1746     |MASTERCARD|DEBIT (PREPAID)|5766121508358701|02/2023|908|YES     |1               |$99         |07/2006       |2012                 |NO              |99.0              |\n",
      "+----+---------+----------+---------------+----------------+-------+---+--------+----------------+------------+--------------+---------------------+----------------+------------------+\n",
      "only showing top 10 rows\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- client_id: integer (nullable = true)\n",
      " |-- card_brand: string (nullable = true)\n",
      " |-- card_type: string (nullable = true)\n",
      " |-- card_number: long (nullable = true)\n",
      " |-- expires: string (nullable = true)\n",
      " |-- cvv: integer (nullable = true)\n",
      " |-- has_chip: string (nullable = true)\n",
      " |-- num_cards_issued: integer (nullable = true)\n",
      " |-- credit_limit: string (nullable = true)\n",
      " |-- acct_open_date: string (nullable = true)\n",
      " |-- year_pin_last_changed: integer (nullable = true)\n",
      " |-- card_on_dark_web: string (nullable = true)\n",
      " |-- credit_limit_clean: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, when, upper, trim\n",
    "\n",
    "# 1. Clean credit limit column\n",
    "cards_clean = cards_df.withColumn(\n",
    "    \"credit_limit_clean\",\n",
    "    regexp_replace(col(\"credit_limit\"), \"[$,]\", \"\").cast(\"double\")\n",
    ")\n",
    "\n",
    "# 2. Fill missing card_brand\n",
    "cards_clean = cards_clean.withColumn(\n",
    "    \"card_brand\",\n",
    "    when(col(\"card_brand\").isNull(), \"Unknown\").otherwise(col(\"card_brand\"))\n",
    ")\n",
    "\n",
    "# 3. Standardize categorical columns\n",
    "cards_clean = cards_clean.withColumn(\"card_brand\", upper(trim(col(\"card_brand\"))))\n",
    "cards_clean = cards_clean.withColumn(\"card_type\", upper(trim(col(\"card_type\"))))\n",
    "cards_clean = cards_clean.withColumn(\"has_chip\", upper(trim(col(\"has_chip\"))))\n",
    "cards_clean = cards_clean.withColumn(\"card_on_dark_web\", upper(trim(col(\"card_on_dark_web\"))))\n",
    "\n",
    "# 4. Ensure numeric columns are proper type\n",
    "cards_clean = cards_clean.withColumn(\"num_cards_issued\", col(\"num_cards_issued\").cast(\"integer\"))\n",
    "\n",
    "# 5. Fill missing has_chip values as 'UNKNOWN'\n",
    "cards_clean = cards_clean.withColumn(\n",
    "    \"has_chip\",\n",
    "    when(col(\"has_chip\").isNull(), \"UNKNOWN\").otherwise(col(\"has_chip\"))\n",
    ")\n",
    "\n",
    "# 6. Preview cleaned data\n",
    "cards_clean.show(10, truncate=False)\n",
    "cards_clean.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a490ed-0ea6-4158-b6c9-7cbf9c899043",
   "metadata": {},
   "source": [
    "#### Standardizing Card type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "324bbdb1-1e23-4314-a1d5-85dd9dc1f466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|card_type_clean|count|\n",
      "+---------------+-----+\n",
      "|          DEBIT| 3485|\n",
      "|         CREDIT| 2044|\n",
      "|DEBIT (PREPAID)|  574|\n",
      "|        UNKNOWN|   43|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Standardize card_type properly\n",
    "cards_clean = cards_clean.withColumn(\n",
    "    \"card_type_clean\",\n",
    "    when(col(\"card_type\").rlike(\"(?i)^DEBIT\\\\s*\\\\(PREPAID\\\\)$\"), \"DEBIT (PREPAID)\")  # Match DEBIT (PREPAID)\n",
    "    .when(col(\"card_type\").rlike(\"(?i)^DEBIT$\"), \"DEBIT\")  # Match exact DEBIT\n",
    "    .when(col(\"card_type\").rlike(\"(?i)^CREDIT$\"), \"CREDIT\")  # Match exact CREDIT\n",
    "    .otherwise(\"UNKNOWN\")  # Any other unknown/misspelled values\n",
    ")\n",
    "\n",
    "# Verify the correction\n",
    "cards_clean.groupBy(\"card_type_clean\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dad639f-b414-49ff-b508-15b31f307791",
   "metadata": {},
   "source": [
    "During the data cleaning here:\n",
    "\n",
    "1. We cleaned the credit_limit columns which was string originally.\n",
    "2. Missing values were checked and no nulls werre found in credit limit or number of cards issues.\n",
    "3. The card_type column had inconsistent spellings and formats such as DEBIT, DE=BIt, CREJDIT, etc. All these entriees were standardized to DEBIT, CREDIT, DEBIT (PREPAID).\n",
    "4. card_brand missing values were replaced with UNKOWN.\n",
    "5. all categorical columns were converted to uppercase to prevent case-sensitive inconsistencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab94c88b-c2dd-49b1-adeb-2115947abdfb",
   "metadata": {},
   "source": [
    "## User's Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ded22f-d645-4ac2-8557-5e0deb9946ef",
   "metadata": {},
   "source": [
    "This dataset contains user-level demographic, financial, and credit-related attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fff6f3dc-a850-4b8e-95ba-470bfab13ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'current_age',\n",
       " 'retirement_age',\n",
       " 'birth_year',\n",
       " 'birth_month',\n",
       " 'gender',\n",
       " 'address',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'per_capita_income',\n",
       " 'yearly_income',\n",
       " 'total_debt',\n",
       " 'credit_score',\n",
       " 'num_credit_cards']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b954f351-53c0-405f-a423-e7d51a553762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+--------------+----------+-----------+------+------------------------+--------+---------+-----------------+-------------+----------+------------+----------------+\n",
      "|id  |current_age|retirement_age|birth_year|birth_month|gender|address                 |latitude|longitude|per_capita_income|yearly_income|total_debt|credit_score|num_credit_cards|\n",
      "+----+-----------+--------------+----------+-----------+------+------------------------+--------+---------+-----------------+-------------+----------+------------+----------------+\n",
      "|825 |53         |66            |1966      |11         |Female|462 Rose Lane           |34.15   |-117.76  |$29278           |$59696       |$127613   |787         |5               |\n",
      "|1746|NULL       |68            |1966      |12         |Female|3606 Federal Boulevard  |40.76   |-73.74   |$37891           |$77254       |$191349   |701         |5               |\n",
      "|1718|81         |67            |1938      |11         |Female|766 Third Drive         |34.02   |-117.89  |$22681           |$33483       |$196      |698         |5               |\n",
      "|708 |63         |63            |1957      |1          |Female|3 Madison Street        |40.71   |-73.99   |$163145          |$249925      |$202328   |722         |4               |\n",
      "|1164|43         |70            |1976      |9          |Male  |9620 Valley Stream Drive|37.76   |-122.44  |$53797           |$109687      |$183855   |675         |1               |\n",
      "|68  |42         |70            |1977      |10         |Male  |58 Birch Lane           |41.55   |-90.6    |$20599           |$41997       |$0        |704         |3               |\n",
      "|1075|NULL       |67            |1983      |12         |Female|5695 Fifth Street       |38.22   |-85.74   |$25258           |$51500       |$102286   |672         |3               |\n",
      "|1711|26         |67            |1993      |12         |Male  |1941 Ninth Street       |45.51   |-122.64  |$26790           |$54623       |$114711   |728         |1               |\n",
      "|1116|NULL       |66            |1938      |7          |Female|11 Spruce Avenue        |40.32   |-75.32   |$26273           |$42509       |$2895     |755         |5               |\n",
      "|1752|34         |60            |1986      |1          |Female|887 Grant Street        |29.97   |-92.12   |$18730           |$38190       |$81262    |810         |1               |\n",
      "+----+-----------+--------------+----------+-----------+------+------------------------+--------+---------+-----------------+-------------+----------+------------+----------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "users_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de955733-e2c2-4fcc-82e4-e4d8d4423850",
   "metadata": {},
   "source": [
    "### 1. Data Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da8dd06-450c-4c21-8a79-f468177bfb72",
   "metadata": {},
   "source": [
    "#### Check Datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "17266bf1-6887-44e4-85b0-56bed85a7d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- current_age: integer (nullable = true)\n",
      " |-- retirement_age: integer (nullable = true)\n",
      " |-- birth_year: integer (nullable = true)\n",
      " |-- birth_month: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- per_capita_income: string (nullable = true)\n",
      " |-- yearly_income: string (nullable = true)\n",
      " |-- total_debt: string (nullable = true)\n",
      " |-- credit_score: integer (nullable = true)\n",
      " |-- num_credit_cards: integer (nullable = true)\n",
      "\n",
      "Total number of rows:  2000\n",
      "+----+-----------+--------------+----------+-----------+------+------------------------+--------+---------+-----------------+-------------+----------+------------+----------------+\n",
      "|id  |current_age|retirement_age|birth_year|birth_month|gender|address                 |latitude|longitude|per_capita_income|yearly_income|total_debt|credit_score|num_credit_cards|\n",
      "+----+-----------+--------------+----------+-----------+------+------------------------+--------+---------+-----------------+-------------+----------+------------+----------------+\n",
      "|825 |53         |66            |1966      |11         |Female|462 Rose Lane           |34.15   |-117.76  |$29278           |$59696       |$127613   |787         |5               |\n",
      "|1746|NULL       |68            |1966      |12         |Female|3606 Federal Boulevard  |40.76   |-73.74   |$37891           |$77254       |$191349   |701         |5               |\n",
      "|1718|81         |67            |1938      |11         |Female|766 Third Drive         |34.02   |-117.89  |$22681           |$33483       |$196      |698         |5               |\n",
      "|708 |63         |63            |1957      |1          |Female|3 Madison Street        |40.71   |-73.99   |$163145          |$249925      |$202328   |722         |4               |\n",
      "|1164|43         |70            |1976      |9          |Male  |9620 Valley Stream Drive|37.76   |-122.44  |$53797           |$109687      |$183855   |675         |1               |\n",
      "|68  |42         |70            |1977      |10         |Male  |58 Birch Lane           |41.55   |-90.6    |$20599           |$41997       |$0        |704         |3               |\n",
      "|1075|NULL       |67            |1983      |12         |Female|5695 Fifth Street       |38.22   |-85.74   |$25258           |$51500       |$102286   |672         |3               |\n",
      "|1711|26         |67            |1993      |12         |Male  |1941 Ninth Street       |45.51   |-122.64  |$26790           |$54623       |$114711   |728         |1               |\n",
      "|1116|NULL       |66            |1938      |7          |Female|11 Spruce Avenue        |40.32   |-75.32   |$26273           |$42509       |$2895     |755         |5               |\n",
      "|1752|34         |60            |1986      |1          |Female|887 Grant Street        |29.97   |-92.12   |$18730           |$38190       |$81262    |810         |1               |\n",
      "+----+-----------+--------------+----------+-----------+------+------------------------+--------+---------+-----------------+-------------+----------+------------+----------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "users_df.printSchema()\n",
    "\n",
    "total_rows = users_df.count()\n",
    "print(\"Total number of rows: \", total_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9db3e7-d380-4d01-a018-76ed71805ed1",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "- Columns related to monetary amounts (per_capita_income, yearly_income, total_debt) are stored as strings with currency symbols and need conversion to numeric.\n",
    "- The dataset contains a mix of categorical (gender) and numerical variables, which will require type standardization.\n",
    "- id is a unique identifier for each user.\n",
    "- Location information (latitude, longitude, address) is available and can be used for spatial analysis or feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2df03df-bf61-408b-a47c-78166c7c4563",
   "metadata": {},
   "source": [
    "#### Checking Duplicacte Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9d862a92-49c2-45bc-a5ee-9935846e5caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|count|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "duplicate_users = users_df.groupBy(\"id\").count().filter(col(\"count\") > 1)\n",
    "duplicate_users.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4e224b-803f-4a48-ae62-347cdbb7cc1f",
   "metadata": {},
   "source": [
    "Observation:\n",
    "\n",
    "- No duplicate user IDs were found.\n",
    "- This ensures that each row represents a unique user, maintaining data integrity for subsequent analysis and feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575363d5-12b4-4d0e-90d4-85976d03a485",
   "metadata": {},
   "source": [
    "#### Categories and Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3d065919-afc1-4188-ba80-2688efaddf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|Female| 1016|\n",
      "|  Male|  984|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Category counts for gender\n",
    "users_df.groupBy(\"gender\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62a1199-2140-4a0d-b080-4b049f00d6c8",
   "metadata": {},
   "source": [
    "This shows that there is no significant imbalance in gender representation in the dataset.\n",
    "There is also no need for standardization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e1b61c-cf0e-49db-a37d-eb7ed8a37d00",
   "metadata": {},
   "source": [
    "#### 2. Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "07af102a-563e-4d82-a664-912cbf6579f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------+----------+-----------+------+-------+--------+---------+-----------------+-------------+----------+------------+----------------+\n",
      "|id |current_age|retirement_age|birth_year|birth_month|gender|address|latitude|longitude|per_capita_income|yearly_income|total_debt|credit_score|num_credit_cards|\n",
      "+---+-----------+--------------+----------+-----------+------+-------+--------+---------+-----------------+-------------+----------+------------+----------------+\n",
      "|0  |21         |3             |30        |0          |0     |0      |0       |0        |0                |0            |0         |0           |0               |\n",
      "+---+-----------+--------------+----------+-----------+------+-------+--------+---------+-----------------+-------------+----------+------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "# Count missing (NULL) values per column\n",
    "missing_df = users_df.select([\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c)\n",
    "    for c in users_df.columns\n",
    "])\n",
    "\n",
    "missing_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8ff632-bc5b-4dbd-be4b-cd7626e51d2e",
   "metadata": {},
   "source": [
    "Observations\n",
    "\n",
    "- Missing values are limited and concentrated in: current_age, retirement_age, birth_year.\n",
    "- We can handle these by imposing some rules and calculations based on eahc of these columns if the dat is present.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e77270-3a99-4a70-bb8e-0088dca74e9e",
   "metadata": {},
   "source": [
    "### 3. Data Integrity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38870dc1-674f-4fae-b960-5082ba05acf0",
   "metadata": {},
   "source": [
    "#### Age consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a34b9236-9fc4-46d5-a633-feefb46dc372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|derived_report_year|count|\n",
      "+-------------------+-----+\n",
      "|               NULL|   47|\n",
      "|               2019| 1573|\n",
      "|               2020|  380|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Derive implied report year\n",
    "users_df.withColumn(\n",
    "    \"derived_report_year\",\n",
    "    col(\"birth_year\") + col(\"current_age\")\n",
    ").groupBy(\"derived_report_year\") \\\n",
    " .count() \\\n",
    " .orderBy(\"derived_report_year\") \\\n",
    " .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9703445c-73a4-4bca-a79b-bb1cbc1c9e31",
   "metadata": {},
   "source": [
    "To verify whether all user records belong to the same reporting year by deriving the dataset year using demographic attributes.\n",
    "\n",
    "`Derived Report Year = birth_year + current_age`\n",
    "\n",
    "The majority of records (≈79%) correspond to report year 2019.\n",
    "A significant subset of records corresponds to report year 2020.\n",
    "This might be due to variations in teh month of birth. \n",
    "We can clearly say that the dat is collected in the year 2019.\n",
    "\n",
    "With this year, we can also calculate the missing birth years or ages.\n",
    "\n",
    "47 records could not produce a derived report year due to missing current_age or birth year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036e25af-8c6e-43d5-b3f4-a6d5b661f1ce",
   "metadata": {},
   "source": [
    "#### Current age vs Retirement age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2edb1a61-10ae-48f5-b788-e6399c990f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+--------------+\n",
      "|  id|current_age|retirement_age|\n",
      "+----+-----------+--------------+\n",
      "|1718|         81|            67|\n",
      "| 708|         63|            63|\n",
      "| 153|         76|            71|\n",
      "|1946|         76|            66|\n",
      "|1674|         70|            64|\n",
      "|1492|         63|            58|\n",
      "|1198|         82|            67|\n",
      "| 898|         90|            66|\n",
      "|  16|         75|            67|\n",
      "|1269|         56|            56|\n",
      "|1913|         60|            57|\n",
      "| 871|         86|            71|\n",
      "|1520|         67|            65|\n",
      "|1372|         76|            66|\n",
      "|1155|         83|            65|\n",
      "| 663|         68|            63|\n",
      "|1987|         63|            62|\n",
      "|1484|         92|            72|\n",
      "|1769|         75|            65|\n",
      "| 906|         75|            69|\n",
      "+----+-----------+--------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Check age vs retirement age consistency\n",
    "users_df.filter(\n",
    "    col(\"current_age\").isNotNull() &\n",
    "    col(\"retirement_age\").isNotNull() &\n",
    "    (col(\"current_age\") >= col(\"retirement_age\"))\n",
    ").select(\n",
    "    \"id\", \"current_age\", \"retirement_age\"\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d5ae945e-cc6a-4cb7-aab4-a9b98a6d8ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|    retirement_age|\n",
      "+-------+------------------+\n",
      "|  count|              1997|\n",
      "|   mean| 66.24186279419129|\n",
      "| stddev|3.6152773885824585|\n",
      "|    min|                50|\n",
      "|    max|                79|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Show summary statistics for retirement_age\n",
    "users_df.select(\"retirement_age\").describe().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7acb71-b9c5-4a4a-8a43-2160393c9fa8",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "Most retirement ages fall within a reasonable range (50–79), with an average of ~66 years.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcf3de2-84b1-4bfe-97e1-6379a39956ec",
   "metadata": {},
   "source": [
    "#### Income vs Debt Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ceffc5a1-a78e-41aa-9d5f-1d9843e54947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+-------------------+--------------------+\n",
      "|  id|total_debt_clean|yearly_income_clean|debt_to_income_ratio|\n",
      "+----+----------------+-------------------+--------------------+\n",
      "| 825|        127613.0|            59696.0|                2.14|\n",
      "|1746|        191349.0|            77254.0|                2.48|\n",
      "|1164|        183855.0|           109687.0|                1.68|\n",
      "|1075|        102286.0|            51500.0|                1.99|\n",
      "|1711|        114711.0|            54623.0|                 2.1|\n",
      "|1752|         81262.0|            38190.0|                2.13|\n",
      "| 640|         94016.0|            45727.0|                2.06|\n",
      "|1679|         89214.0|            69149.0|                1.29|\n",
      "|1094|         78833.0|            41442.0|                 1.9|\n",
      "|1590|         32509.0|            20513.0|                1.58|\n",
      "|1747|         38333.0|            36497.0|                1.05|\n",
      "| 429|         89056.0|            53995.0|                1.65|\n",
      "| 511|         55369.0|            35602.0|                1.56|\n",
      "| 309|         43205.0|            25122.0|                1.72|\n",
      "| 482|         63849.0|            34929.0|                1.83|\n",
      "| 877|         61826.0|            34317.0|                 1.8|\n",
      "| 128|         59615.0|            31377.0|                 1.9|\n",
      "| 775|        104052.0|            43638.0|                2.38|\n",
      "|1536|         38260.0|            32531.0|                1.18|\n",
      "| 140|         72801.0|            42120.0|                1.73|\n",
      "+----+----------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "users_clean = users_clean.withColumn(\n",
    "    \"debt_to_income_ratio\",\n",
    "    round(col(\"total_debt_clean\") / col(\"yearly_income_clean\"), 2)\n",
    ")\n",
    "\n",
    "users_clean.select(\"id\", \"total_debt_clean\", \"yearly_income_clean\", \"debt_to_income_ratio\")\\\n",
    "    .filter(col(\"debt_to_income_ratio\") > 1)\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddc9b5a-306d-47d2-9e01-257b674223e0",
   "metadata": {},
   "source": [
    "We computed the Debt-to-Income Ratio for each user as:\n",
    "\n",
    "Debt-to-Income Ratio = total_debt_clean/yearly_income_clean\n",
    "\n",
    "Users with unusually high ratios (>1) are flagged as potential inconsistencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917c3c86-c7d1-4f18-9c2d-3870768c8b63",
   "metadata": {},
   "source": [
    "#### Sanity Check for Income Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8d16ba79-12d2-40e9-a6f9-6cc19db0be75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+-------------------+----------------+\n",
      "| id|per_capita_income_clean|yearly_income_clean|total_debt_clean|\n",
      "+---+-----------------------+-------------------+----------------+\n",
      "+---+-----------------------+-------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# Remove $ and commas\n",
    "users_clean = users_df.withColumn(\n",
    "    \"per_capita_income_clean\",\n",
    "    regexp_replace(col(\"per_capita_income\"), \"[$,]\", \"\").cast(\"double\")\n",
    ").withColumn(\n",
    "    \"yearly_income_clean\",\n",
    "    regexp_replace(col(\"yearly_income\"), \"[$,]\", \"\").cast(\"double\")\n",
    ").withColumn(\n",
    "    \"total_debt_clean\",\n",
    "    regexp_replace(col(\"total_debt\"), \"[$,]\", \"\").cast(\"double\")\n",
    ")\n",
    "\n",
    "# Check for negative or extreme values\n",
    "users_clean.select(\n",
    "    \"id\", \"per_capita_income_clean\", \"yearly_income_clean\", \"total_debt_clean\"\n",
    ").filter(\n",
    "    (col(\"per_capita_income_clean\") < 0) |\n",
    "    (col(\"yearly_income_clean\") < 0) |\n",
    "    (col(\"total_debt_clean\") < 0)\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11f1619-3733-4e32-b6a7-4101388d6522",
   "metadata": {},
   "source": [
    "All income and debt columns were cleaned to remove $ and , symbols. No negative or missing values remain after cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbddc851-5ae1-4550-8b28-5d707b8342a7",
   "metadata": {},
   "source": [
    "#### Summary of columns to check for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "92b1f5a8-36b8-46e3-8f3b-4da99d19a655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+-------------------+-----------------+\n",
      "|summary|per_capita_income_clean|yearly_income_clean| total_debt_clean|\n",
      "+-------+-----------------------+-------------------+-----------------+\n",
      "|  count|                   2000|               2000|             2000|\n",
      "|   mean|              23141.928|          45715.882|        63709.694|\n",
      "| stddev|      11324.13735766499|  22992.61545631198|52254.45342050286|\n",
      "|    min|                    0.0|                1.0|              0.0|\n",
      "|    max|               163145.0|           307018.0|         516263.0|\n",
      "+-------+-----------------------+-------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_clean.select(\n",
    "    \"per_capita_income_clean\", \"yearly_income_clean\", \"total_debt_clean\"\n",
    ").describe().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f44b9ab-cfbe-4d0e-80da-80cc964c8820",
   "metadata": {},
   "source": [
    "#### Credit Score Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "28920311-ea7e-4f40-9bc6-8693077be1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "| id|credit_score|\n",
      "+---+------------+\n",
      "+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_clean.select(\"id\", \"credit_score\").filter(\n",
    "    (col(\"credit_score\") < 450) | (col(\"credit_score\") > 850)\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd3c38-0303-4de0-83a7-fe88854c0c65",
   "metadata": {},
   "source": [
    "All values in credit_score were within expected ranges (typically 300–850), with no missing or out-of-range values detected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ea4c2-c78e-4cce-b28c-88ae93a48b88",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5c69b6b5-4b27-401f-90e3-7c65eaf49c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# from our previous calculations\n",
    "report_year = 2019\n",
    "\n",
    "# Calculate median retirement age and median current age\n",
    "median_retirement_age = users_df.approxQuantile(\"retirement_age\", [0.5], 0.01)[0]\n",
    "median_current_age = users_df.approxQuantile(\"current_age\", [0.5], 0.01)[0]\n",
    "\n",
    "# Impute missing values\n",
    "users_df = users_df.withColumn(\n",
    "    \"current_age\",\n",
    "    when(col(\"current_age\").isNull() & col(\"birth_year\").isNotNull(), report_year - col(\"birth_year\"))\n",
    "    .when(col(\"current_age\").isNull() & col(\"birth_year\").isNull(), median_current_age)\n",
    "    .otherwise(col(\"current_age\"))\n",
    ").withColumn(\n",
    "    \"birth_year\",\n",
    "    when(col(\"birth_year\").isNull() & col(\"current_age\").isNotNull(), report_year - col(\"current_age\"))\n",
    "    .when(col(\"birth_year\").isNull() & col(\"current_age\").isNull(), report_year - median_current_age)\n",
    "    .otherwise(col(\"birth_year\"))\n",
    ").withColumn(\n",
    "    \"retirement_age\",\n",
    "    when(col(\"retirement_age\").isNull(), median_retirement_age)\n",
    "    .otherwise(col(\"retirement_age\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf83db3-2ddb-4dfe-9f23-0592222f5d89",
   "metadata": {},
   "source": [
    "Imputing `current_age`, `birth_year`:\n",
    "\n",
    "        If current_age is missing but birth_year is available:\n",
    "\n",
    "        current_age = report_year − birth_year\n",
    "\n",
    "        If birth_year is missing but current_age is available:\n",
    "\n",
    "            birth_year = report_year − current_age\n",
    "\n",
    "If both current_age and birth_year are missing, impute using median current age.\n",
    "\n",
    "Impute missing `retirement_age`:\n",
    "\n",
    "    Missing values filled with median retirement age of the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c52a508c-b90b-4f14-bc15-4e7169373ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------+----------+-----------+------+-------+--------+---------+-----------------+-------------+----------+------------+----------------+\n",
      "|id |current_age|retirement_age|birth_year|birth_month|gender|address|latitude|longitude|per_capita_income|yearly_income|total_debt|credit_score|num_credit_cards|\n",
      "+---+-----------+--------------+----------+-----------+------+-------+--------+---------+-----------------+-------------+----------+------------+----------------+\n",
      "|0  |0          |0             |0         |0          |0     |0      |0       |0        |0                |0            |0         |0           |0               |\n",
      "+---+-----------+--------------+----------+-----------+------+-------+--------+---------+-----------------+-------------+----------+------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "# Count missing (NULL) values per column\n",
    "missing_df = users_df.select([\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c)\n",
    "    for c in users_df.columns\n",
    "])\n",
    "\n",
    "missing_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247d2db3-f951-4909-93d9-0ee73a9a36ca",
   "metadata": {},
   "source": [
    "All users now have non-null values for current_age, birth_year, and retirement_age.\n",
    "This ensures consistent age-related calculations and supports further analysis like age vs. retirement age consistency checks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a3ebba-165b-4eb5-ac6c-17eb5b115ee9",
   "metadata": {},
   "source": [
    "#### Type casting columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "15297ba8-1576-4419-97e8-15b25e97e64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+----------------+\n",
      "|per_capita_income_clean|yearly_income_clean|total_debt_clean|\n",
      "+-----------------------+-------------------+----------------+\n",
      "|                29278.0|            59696.0|        127613.0|\n",
      "|                37891.0|            77254.0|        191349.0|\n",
      "|                22681.0|            33483.0|           196.0|\n",
      "|               163145.0|           249925.0|        202328.0|\n",
      "|                53797.0|           109687.0|        183855.0|\n",
      "|                20599.0|            41997.0|             0.0|\n",
      "|                25258.0|            51500.0|        102286.0|\n",
      "|                26790.0|            54623.0|        114711.0|\n",
      "|                26273.0|            42509.0|          2895.0|\n",
      "|                18730.0|            38190.0|         81262.0|\n",
      "+-----------------------+-------------------+----------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# Clean financial columns by removing $ and commas, then cast to double\n",
    "financial_cols = [\"per_capita_income\", \"yearly_income\", \"total_debt\"]\n",
    "\n",
    "for col_name in financial_cols:\n",
    "    users_df = users_df.withColumn(\n",
    "        f\"{col_name}_clean\",\n",
    "        regexp_replace(col(col_name), \"[$,]\", \"\").cast(\"double\")\n",
    "    )\n",
    "\n",
    "# Verify cleaning\n",
    "users_df.select([f\"{c}_clean\" for c in financial_cols]).show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61385025-8e17-4026-8546-93be93b41327",
   "metadata": {},
   "source": [
    "Removing non-numeric characters:\n",
    "\n",
    "    $ signs and commas removed from all financial columns.\n",
    "\n",
    "Convert to numeric type:\n",
    "\n",
    "    Columns cast to double for proper numeric calculations.\n",
    "\n",
    "All users now have clean numeric values in per_capita_income_clean, yearly_income_clean, and total_debt_clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3d7ed792-5f23-441e-a38f-3bf18d00e00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------+----------+-----------+------+-------+--------+---------+-----------------+-------------+----------+------------+----------------+-----------------------+-------------------+----------------+\n",
      "| id|current_age|retirement_age|birth_year|birth_month|gender|address|latitude|longitude|per_capita_income|yearly_income|total_debt|credit_score|num_credit_cards|per_capita_income_clean|yearly_income_clean|total_debt_clean|\n",
      "+---+-----------+--------------+----------+-----------+------+-------+--------+---------+-----------------+-------------+----------+------------+----------------+-----------------------+-------------------+----------------+\n",
      "+---+-----------+--------------+----------+-----------+------+-------+--------+---------+-----------------+-------------+----------+------------+----------------+-----------------------+-------------------+----------------+\n",
      "\n",
      "invalid geogrpahic locations:  None\n",
      "+--------------+---------------+\n",
      "|latitude_clean|longitude_clean|\n",
      "+--------------+---------------+\n",
      "|         34.15|        -117.76|\n",
      "|         40.76|         -73.74|\n",
      "|         34.02|        -117.89|\n",
      "|         40.71|         -73.99|\n",
      "|         37.76|        -122.44|\n",
      "|         41.55|          -90.6|\n",
      "|         38.22|         -85.74|\n",
      "|         45.51|        -122.64|\n",
      "|         40.32|         -75.32|\n",
      "|         29.97|         -92.12|\n",
      "+--------------+---------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Check for invalid latitude/longitude\n",
    "invalid_lat_lon = users_df.filter(\n",
    "    (col(\"latitude\") < -90) | (col(\"latitude\") > 90) |\n",
    "    (col(\"longitude\") < -180) | (col(\"longitude\") > 180)\n",
    ")\n",
    "\n",
    "print(\"invalid geogrpahic locations: \", invalid_lat_lon.show())\n",
    "\n",
    "users_clean = users_df.withColumn(\n",
    "    \"latitude_clean\",\n",
    "    when((col(\"latitude\") >= -90) & (col(\"latitude\") <= 90), col(\"latitude\"))\n",
    "    .otherwise(None)\n",
    ").withColumn(\n",
    "    \"longitude_clean\",\n",
    "    when((col(\"longitude\") >= -180) & (col(\"longitude\") <= 180), col(\"longitude\"))\n",
    "    .otherwise(None)\n",
    ")\n",
    "\n",
    "# Verify cleaned columns\n",
    "users_clean.select(\"latitude_clean\", \"longitude_clean\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ec2773-85b7-467d-8d9d-900b559617f6",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76ee187-dbc3-4bd4-baee-6bd6c13b7a60",
   "metadata": {},
   "source": [
    "#### Debt-to-Income Ratio\n",
    "Calculated as total_debt / yearly_income for each user.\n",
    "\n",
    "Provides a measure of financial risk: higher ratios indicate that a larger portion of income is consumed by debt obligations.\n",
    "\n",
    "Column added to the cleaned dataset as debt_to_income_ratio.\n",
    "THis shows if a usesr is at financial risk or stable situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7cc74008-956b-4760-a001-e872c39cbf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+-------------------+--------------------+\n",
      "|  id|total_debt_clean|yearly_income_clean|debt_to_income_ratio|\n",
      "+----+----------------+-------------------+--------------------+\n",
      "| 825|        127613.0|            59696.0|                2.14|\n",
      "|1746|        191349.0|            77254.0|                2.48|\n",
      "|1718|           196.0|            33483.0|                0.01|\n",
      "| 708|        202328.0|           249925.0|                0.81|\n",
      "|1164|        183855.0|           109687.0|                1.68|\n",
      "|  68|             0.0|            41997.0|                 0.0|\n",
      "|1075|        102286.0|            51500.0|                1.99|\n",
      "|1711|        114711.0|            54623.0|                 2.1|\n",
      "|1116|          2895.0|            42509.0|                0.07|\n",
      "|1752|         81262.0|            38190.0|                2.13|\n",
      "+----+----------------+-------------------+--------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, round\n",
    "\n",
    "users_clean = users_clean.withColumn(\n",
    "    \"debt_to_income_ratio\",\n",
    "    round(col(\"total_debt_clean\") / col(\"yearly_income_clean\"), 2)\n",
    ")\n",
    "\n",
    "users_clean.select(\"id\", \"total_debt_clean\", \"yearly_income_clean\", \"debt_to_income_ratio\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdd834c-4ec7-459f-8e82-44d07e54d4a4",
   "metadata": {},
   "source": [
    "#### Credit Score Segments\n",
    "\n",
    "Users were grouped into categories based on their numeric credit score:\n",
    "\n",
    "        Poor: <580\n",
    "        Fair: 580–669\n",
    "        Good: 670–739\n",
    "        Very Good: 740–799\n",
    "        Excellent: 800+\n",
    "\n",
    "This segmentation helps identify different credit risk profiles among users.\n",
    "\n",
    "Added as a new column credit_score_segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "af86c083-b1aa-4279-8ac1-76ef5ef3f1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|credit_score_segment|count|\n",
      "+--------------------+-----+\n",
      "|                Fair|  348|\n",
      "|           Very Good|  474|\n",
      "|           Excellent|  166|\n",
      "|                Good|  931|\n",
      "|                Poor|   81|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Create credit score categories\n",
    "users_clean = users_clean.withColumn(\n",
    "    \"credit_score_segment\",\n",
    "    when(col(\"credit_score\") < 580, \"Poor\")\n",
    "    .when((col(\"credit_score\") >= 580) & (col(\"credit_score\") < 670), \"Fair\")\n",
    "    .when((col(\"credit_score\") >= 670) & (col(\"credit_score\") < 740), \"Good\")\n",
    "    .when((col(\"credit_score\") >= 740) & (col(\"credit_score\") < 800), \"Very Good\")\n",
    "    .otherwise(\"Excellent\")\n",
    ")\n",
    "\n",
    "users_clean.groupBy(\"credit_score_segment\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0f6d8f-b541-4994-aa89-67dfc45abc76",
   "metadata": {},
   "source": [
    "#### Financial Behavior Flags\n",
    "\n",
    "High Debt Flag:\n",
    "    \n",
    "    Users with a debt-to-income ratio > 2.0 are marked as high debt (high_debt_flag = 1).\n",
    "    Indicates potentially risky financial behavior.\n",
    "\n",
    "High Income, Low Debt Flag:\n",
    "\n",
    "    Users with yearly income > 75,000 and debt-to-income ratio < 0.5 are marked as financially stable (high_income_low_debt_flag = 1).\n",
    "\n",
    "These flags can help segment users for risk assessment and targeted financial interventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dc3e673a-1de2-41bd-a7ec-381e3ae665cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|high_debt_flag|count|\n",
      "+--------------+-----+\n",
      "|             1|  470|\n",
      "|             0| 1530|\n",
      "+--------------+-----+\n",
      "\n",
      "+-------------------------+-----+\n",
      "|high_income_low_debt_flag|count|\n",
      "+-------------------------+-----+\n",
      "|                        1|   33|\n",
      "|                        0| 1967|\n",
      "+-------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# High debt flag: debt-to-income > 2.0\n",
    "users_clean = users_clean.withColumn(\n",
    "    \"high_debt_flag\",\n",
    "    when(col(\"debt_to_income_ratio\") > 2.0, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# High income, low debt flag: yearly_income > 75000 & debt-to-income < 0.5\n",
    "users_clean = users_clean.withColumn(\n",
    "    \"high_income_low_debt_flag\",\n",
    "    when((col(\"yearly_income_clean\") > 75000) & (col(\"debt_to_income_ratio\") < 0.5), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Show counts for flags\n",
    "users_clean.groupBy(\"high_debt_flag\").count().show()\n",
    "users_clean.groupBy(\"high_income_low_debt_flag\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f07014-9914-4602-8c3c-e008c2c69837",
   "metadata": {},
   "source": [
    "Further cleaning and feature Engineering required for the data pipeline is continued in the other jupyter file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b335f5-12a2-4827-b7b3-48cc0746e6b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
